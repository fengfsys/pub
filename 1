- name: Show local file content
  debug:
    msg: "{{ lookup('file', '/path/to/file.txt') }}"
    
    import sched
import time
import datetime
import subprocess

scheduler = sched.scheduler(time.time, time.sleep)

def is_weekday():
    return datetime.datetime.today().weekday() < 5  # 0 = Monday

def get_seconds_until(hour, minute):
    now = datetime.datetime.now()
    target = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
    if target <= now:
        target += datetime.timedelta(days=1)
    return (target - now).total_seconds()

def job():
    now = datetime.datetime.now()
    if is_weekday():
        print(f"[{now}] Running scheduled job...")
        subprocess.run("mytask arg1 arg2 &", shell=True)
    else:
        print(f"[{now}] Skipped (not a weekday).")

    # Schedule next run
    scheduler.enter(get_seconds_until(19, 55), 1, job)

# Schedule the first run
scheduler.enter(get_seconds_until(19, 55), 1, job)

print("Task scheduled for 19:55 every weekday.")
scheduler.run()



import schedule
import time
import subprocess
import datetime

def job():
    print(f"[{datetime.datetime.now()}] Running scheduled job...")
    try:
        subprocess.run("mytask arg1 arg2 &", shell=True, check=True)
        print("✅ Job launched in background.")
    except subprocess.CalledProcessError as e:
        print(f"❌ Job failed: {e}")

schedule.every().monday.at("19:55").do(job)
schedule.every().tuesday.at("19:55").do(job)
schedule.every().wednesday.at("19:55").do(job)
schedule.every().thursday.at("19:55").do(job)
schedule.every().friday.at("19:55").do(job)

print("📅 Weekday Scheduler Started. Job set for 19:55 every weekday.")
while True:
    schedule.run_pending()
    time.sleep(1)



@V$hNpKjbR6Vag3

#test git fetch

7s#L9@mQv!2E



# Extract messages as strings
messages_df = raw_df.selectExpr("cast(value as string) as message")

# Define the delimiter pattern (comma followed by one or more spaces)
delimiter_pattern = r",\s+"

# Split the message into lines (if needed) and then into key-value pairs
lines_df = messages_df.select(F.explode(F.split("message", "\n")).alias("line"))
kv_pairs_df = lines_df.select(F.explode(F.split("line", delimiter_pattern)).alias("kv_pair"))

# Filter out non-key-value pairs and extract the fields of interest
filtered_kv_pairs_df = kv_pairs_df.filter(~F.col("kv_pair").rlike("^\\s*$")) \
                                 .withColumn("kv_split", F.split("kv_pair", "\\s+")) \
                                 .filter(F.size("kv_split") == 2) \
                                 .select(F.col("kv_split")[0].alias("key"), F.col("kv_split")[1].alias("value"))

# Filter for the fields of interest
interesting_fields_df = filtered_kv_pairs_df.filter(F.col("key").isin("uid", "name", "hostname"))

# Prefix the value with its key and deduplicate fields while keeping distinct records
prefixed_fields_df = interesting_fields_df.withColumn("prefixed_value", F.concat(F.col("key"), F.lit(": "), F.col("value"))) \
                                         .select("prefixed_value") \
                                         .dropDuplicates()

# Group by prefixed fields and count occurrences
field_counts = prefixed_fields_df.groupBy("prefixed_value").count()

# Start the streaming query
query = field_counts.writeStream.outputMode("complete").format("console").start()

# Wait for the query to terminate (in practice, you'd add a termination condition)
query.awaitTermination()
