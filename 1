import schedule
import time
import subprocess
import datetime

def job():
    print(f"[{datetime.datetime.now()}] Running scheduled job...")
    try:
        subprocess.run("mytask arg1 arg2 &", shell=True, check=True)
        print("‚úÖ Job launched in background.")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Job failed: {e}")

schedule.every().monday.at("19:55").do(job)
schedule.every().tuesday.at("19:55").do(job)
schedule.every().wednesday.at("19:55").do(job)
schedule.every().thursday.at("19:55").do(job)
schedule.every().friday.at("19:55").do(job)

print("üìÖ Weekday Scheduler Started. Job set for 19:55 every weekday.")
while True:
    schedule.run_pending()
    time.sleep(1)



@V$hNpKjbR6Vag3

#test git fetch

7s#L9@mQv!2E



# Extract messages as strings
messages_df = raw_df.selectExpr("cast(value as string) as message")

# Define the delimiter pattern (comma followed by one or more spaces)
delimiter_pattern = r",\s+"

# Split the message into lines (if needed) and then into key-value pairs
lines_df = messages_df.select(F.explode(F.split("message", "\n")).alias("line"))
kv_pairs_df = lines_df.select(F.explode(F.split("line", delimiter_pattern)).alias("kv_pair"))

# Filter out non-key-value pairs and extract the fields of interest
filtered_kv_pairs_df = kv_pairs_df.filter(~F.col("kv_pair").rlike("^\\s*$")) \
                                 .withColumn("kv_split", F.split("kv_pair", "\\s+")) \
                                 .filter(F.size("kv_split") == 2) \
                                 .select(F.col("kv_split")[0].alias("key"), F.col("kv_split")[1].alias("value"))

# Filter for the fields of interest
interesting_fields_df = filtered_kv_pairs_df.filter(F.col("key").isin("uid", "name", "hostname"))

# Prefix the value with its key and deduplicate fields while keeping distinct records
prefixed_fields_df = interesting_fields_df.withColumn("prefixed_value", F.concat(F.col("key"), F.lit(": "), F.col("value"))) \
                                         .select("prefixed_value") \
                                         .dropDuplicates()

# Group by prefixed fields and count occurrences
field_counts = prefixed_fields_df.groupBy("prefixed_value").count()

# Start the streaming query
query = field_counts.writeStream.outputMode("complete").format("console").start()

# Wait for the query to terminate (in practice, you'd add a termination condition)
query.awaitTermination()
