# Extract messages as strings
messages_df = raw_df.selectExpr("cast(value as string) as message")

# Define the delimiter pattern (comma followed by one or more spaces)
delimiter_pattern = r",\s+"

# Split the message into lines (if needed) and then into key-value pairs
lines_df = messages_df.select(F.explode(F.split("message", "\n")).alias("line"))
kv_pairs_df = lines_df.select(F.explode(F.split("line", delimiter_pattern)).alias("kv_pair"))

# Filter out non-key-value pairs and extract the fields of interest
filtered_kv_pairs_df = kv_pairs_df.filter(~F.col("kv_pair").rlike("^\\s*$")) \
                                 .withColumn("kv_split", F.split("kv_pair", "\\s+")) \
                                 .filter(F.size("kv_split") == 2) \
                                 .select(F.col("kv_split")[0].alias("key"), F.col("kv_split")[1].alias("value"))

# Filter for the fields of interest and add a batch ID
batch_df = filtered_kv_pairs_df.withColumn("batch_id", F.spark_partition_id())

# Create a window specification to assign a row number within each batch_id and key-value pair
window_spec = Window.partitionBy("batch_id", "key", "value").orderBy(lit(1))

# Add a row number and keep only the first occurrence within each batch
deduplicated_df = batch_df.withColumn("row_num", row_number().over(window_spec)) \
                          .where(col("row_num") == 1) \
                          .drop("row_num")

# Prefix the value with its key
prefixed_fields_df = deduplicated_df.filter(F.col("key").isin("uid", "name", "hostname")) \
                                   .withColumn("prefixed_value", F.concat(F.col("key"), F.lit(": "), F.col("value")))

# Count occurrences
field_counts = prefixed_fields_df.groupBy("prefixed_value").count()

# Start the streaming query
query = field_counts.writeStream.outputMode("complete").format("console").start()

# Wait for the query to terminate (in practice, you'd add a termination condition)
query.awaitTermination()
