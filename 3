# Extract messages as strings
messages_df = raw_df.selectExpr("cast(value as string) as message")

# Define the delimiter pattern (comma followed by one or more spaces)
delimiter_pattern = r",\s+"

# Split the message into lines (if needed) and then into key-value pairs
lines_df = messages_df.select(F.explode(F.split("message", "\n")).alias("line"))
kv_pairs_df = lines_df.select(F.explode(F.split("line", delimiter_pattern)).alias("kv_pair"))

# Filter out non-key-value pairs and extract the fields of interest
filtered_kv_pairs_df = kv_pairs_df.filter(~F.col("kv_pair").rlike("^\\s*$")) \
                                 .withColumn("kv_split", F.split("kv_pair", "\\s+")) \
                                 .filter(F.size("kv_split") == 2) \
                                 .select(F.col("kv_split")[0].alias("key"), F.col("kv_split")[1].alias("value"))

# Filter for the fields of interest, flatten into a single column, and create prefixed values
prefixed_fields_df = filtered_kv_pairs_df.filter(F.col("key").isin("uid", "name", "hostname")) \
                                       .withColumn("prefixed_value", F.concat(F.col("key"), F.lit(": "), F.col("value"))) \
                                       .select("prefixed_value")

# Use distinct to remove duplicates within each batch
distinct_values_df = prefixed_fields_df.distinct()

# Count occurrences
field_counts = distinct_values_df.groupBy("prefixed_value").count()

# Start the streaming query
query = field_counts.writeStream.outputMode("complete").format("console").start()

# Wait for the query to terminate (in practice, you'd add a termination condition)
query.awaitTermination()
